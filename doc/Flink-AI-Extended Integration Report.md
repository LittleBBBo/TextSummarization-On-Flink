## Flink-AI-Extended Integration Report

[TOC]

在7.15-7.26这段时间内，我基于Flink ML pipeline框架实现了两个通用的class：**TFEstimator**和**TFModel**，这两个class分别在**fit()**和**transform()**方法里封装了Flink-AI-Extended的**train**和**inference**过程，通过**WithParams**接口实现通用参数的配置和传递。

截至7.26，独立的TFEstimator训练并返回TFModel、独立的TFModel引用均已完成功能上的实现与测试，并通过两个Kafka topic作为source和sink构建出一个简单的命令行end-to-end应用。

**但是**，将TFEstimator作为一个**PipelineStage**加入到Pipeline并进行训练时，遇到**Blocker**，原因是pipeline.fit()需要先调用TFEstimator.fit()产生一个TFModel，再用该TFModel来transform输入的table传给下一个PipelineStage，相当于在同一个job里调用Flink-AI-Extended的**train**和**inference**过程，这在目前是不支持的，详见Issue-1。

本文将详细介绍在Flink-AI-Extended到Flink ML pipeline框架的整合过程中 ：

1. 设计实现上的细节；
2. 遇到的问题、发生的场景；

3. 可能的改进

### 1. Implementation of TFEstimator & TFModel

TFEstimator与TFModel是一个具体的实现类，理论上能运行任何基于Flink-AI-Extended扩展的TF算法，通过一些通用化的参数配置将Flink-AI-Extended的调用过程完全封装起来，目的是让用户仅从自己对TF的知识就能简单的构造一个TF算法的estimator或model。

#### Params

一般来说，使用Flink-AI-Extended时需要配置以下几类参数：

- cluster信息：包括zookeeper地址、worker数量、ps数量
- 输入输出信息：包括input table中需要传给TF的列名、TF返回flink的output table中的列名与对应类型
- python运行信息：包括所有python文件路径、主函数入口、传给python的超参、虚拟环境路径

因此，对每一类的参数设计了统一的接口，并让TFEstimator、TFModel实现了如下这些接口。需要注意的是，对于输入输出和python相关的参数，设计了两套相同的接口分别给Training和Inference过程使用。这样设计是因为虽然TFEstimator与TFModel在一般应用中大部分参数应该是一样的，但无法强制要求用户一定要按这样的规范去开发TF算法，所以保留了给TFEstimator和TFModel独立配置一整套参数的能力。比如说，用户在训练过程中的入口函数是"train_on_flink"，而在引用过程可以是"inference_on_flink"。

```java
package org.apache.flink.table.ml.lib.tensorflow.param;

/**
 * Parameters for cluster configuration, including:
 * 1. zookeeper address
 * 2. worker number
 * 3. ps number
 */
public interface HasClusterConfig<T> extends WithParams<T> {
    ParamInfo<String> ZOOKEEPER_CONNECT_STR;
    ParamInfo<Integer> WORKER_NUM;
    ParamInfo<Integer> PS_NUM;
}


/**
 * Parameters for python configuration in training process, including:
 * 1. paths of python scripts
 * 2. entry function in main python file
 * 3. key to get hyper parameter in python
 * 4. hyper parameter for python
 * 5. virtual environment path
 */
public interface HasTrainPythonConfig<T> extends WithParams<T> {
    ParamInfo<String[]> TRAIN_SCRIPTS;
    ParamInfo<String> TRAIN_MAP_FUNC;
  	ParamInfo<String> TRAIN_HYPER_PARAMS_KEY;
    ParamInfo<String[]> TRAIN_HYPER_PARAMS;
    ParamInfo<String> TRAIN_ENV_PATH;
}


/**
 * An interface for classes with a parameter specifying 
 * the name of multiple selected input columns.
 */
public interface HasTrainSelectedCols<T> extends WithParams<T> {
    ParamInfo<String[]> TRAIN_SELECTED_COLS;
}


/**
 * An interface for classes with a parameter specifying 
 * the names of multiple output columns.
 */
public interface HasTrainOutputCols<T> extends WithParams<T> {
    ParamInfo<String[]> TRAIN_OUTPUT_COLS;
}


/**
 * An interface for classes with a parameter specifying
 * the types of multiple output columns.
 */
public interface HasTrainOutputTypes<T> extends WithParams<T> {
    ParamInfo<DataTypes[]> TRAIN_OUTPUT_TYPES;
}


/**
 * Mirrored interfaces for configuration in inference process
 */
public interface HasInferencePythonConfig<T> extends WithParams<T>;
public interface HasInferenceSelectedCols<T> extends WithParams<T>;
public interface HasInferenceOutputCols<T> extends WithParams<T>;
public interface HasInferenceOutputTypes<T> extends WithParams<T>;
```

#### TFModel

通用的TFModel通过HasClusterConfig、HasInferencePythonConfig接口配置集群信息、Python相关信息（如文件路径、超参等）；通过HasInferenceSelectedCols、HasInferenceOutputCols、HasInferenceOutputTypes接口配置与TF进行数据传输相关的encoding、decoding格式。

```java
/**
 * A general TensorFlow model implemented by Flink-AI-Extended,
 * is usually generated by an {@link TFEstimator}
 * when {@link TFEstimator#fit(TableEnvironment, Table)} is invoked.
 */
public class TFModel implements Model<TFModel>, 
		HasClusterConfig<TFModel>, 
		HasInferencePythonConfig<TFModel>, 
		HasInferenceSelectedCols<TFModel>, 
		HasInferenceOutputCols<TFModel>, 
		HasInferenceOutputTypes<TFModel> {
      
    private static final Logger LOG = LoggerFactory.getLogger(TFModel.class);
    private Params params = new Params();

    @Override
    public Table transform(TableEnvironment tableEnvironment, Table table) {
        StreamExecutionEnvironment streamEnv;
        try {
          	// TODO: [hack] transform table to dataStream to get StreamExecutionEnvironment
            if (tableEnvironment instanceof StreamTableEnvironment) {
                StreamTableEnvironment streamTableEnvironment = 
                  	(StreamTableEnvironment)tableEnvironment;
                streamEnv = streamTableEnvironment
                  	.toAppendStream(table, Row.class).getExecutionEnvironment();
            } else {
                throw new RuntimeException("Unsupported TableEnvironment, please use StreamTableEnvironment");
            }
          	
          	// Select the necessary columns according to "SelectedCols"
            Table inputTable = configureInputTable(table);
          	// Construct the output schema according on the "OutputCols" and "OutputTypes"
            TableSchema outputSchema = configureOutputSchema();
          	// Create a basic TFConfig according to "ClusterConfig" and "PythonConfig"
            TFConfig config = configureTFConfig();
          	// Configure the row encoding and decoding base on input & output schema
            configureExampleCoding(config, inputTable.getSchema(), outputSchema);
          	// transform the table by TF which implemented by AI-Extended
            Table outputTable = TFUtils
              	.inference(streamEnv, tableEnvironment, inputTable, config, outputSchema);
            return outputTable;
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    @Override
    public Params getParams() {
        return params;
    }
}
```

#### TFEstimator

通用的TFEstimator参数配置过程与TFModel类似，但需要同时配置Train过程和Inference过程的参数，因为TFEstimator需要返回一个实例化的TFModel，两者大部分参数应该是一样的，但无法要求用户一定要按这样的规范去开发TF算法，所以保留了给TFEstimator和TFModel独立配置一整套参数的能力。

```java
/**
 * A general TensorFlow estimator implemented by Flink-AI-Extended,
 * responsible for training and generating TensorFlow models.
 */
public class TFEstimator implements Estimator<TFEstimator, TFModel>, 
		HasClusterConfig<TFEstimator>,

    HasTrainPythonConfig<TFEstimator>, 
		HasInferencePythonConfig<TFEstimator>,

    HasTrainSelectedCols<TFEstimator>, 
		HasTrainOutputCols<TFEstimator>, 
		HasTrainOutputTypes<TFEstimator>,

    HasInferenceSelectedCols<TFEstimator>, 
		HasInferenceOutputCols<TFEstimator>, 
		HasInferenceOutputTypes<TFEstimator> {
      
    private Params params = new Params();

    @Override
    public TFModel fit(TableEnvironment tableEnvironment, Table table) {
        StreamExecutionEnvironment streamEnv;
        try {
          	// TODO: [hack] transform table to dataStream to get StreamExecutionEnvironment
            if (tableEnvironment instanceof StreamTableEnvironment) {
                StreamTableEnvironment streamTableEnvironment = 
                  	(StreamTableEnvironment)tableEnvironment;
                streamEnv = streamTableEnvironment
                  	.toAppendStream(table, Row.class).getExecutionEnvironment();
            } else {
                throw new RuntimeException("Unsupported TableEnvironment, please use StreamTableEnvironment");
            }
          	// Select the necessary columns according to "SelectedCols"
            Table inputTable = configureInputTable(table);
          	// Construct the output schema according on the "OutputCols" and "OutputTypes"
            TableSchema outputSchema = configureOutputSchema();
          	// Create a basic TFConfig according to "ClusterConfig" and "PythonConfig"
            TFConfig config = configureTFConfig();
          	// Configure the row encoding and decoding base on input & output schema
            configureExampleCoding(config, inputTable.getSchema(), outputSchema);
          	// transform the table by TF which implemented by AI-Extended
            Table outputTable = TFUtils.train(streamEnv, tableEnvironment, inputTable, config, outputSchema);
						// Construct the trained model by inference related config
            TFModel model = new TFModel()
                    .setZookeeperConnStr(getZookeeperConnStr())
                    .setWorkerNum(getWorkerNum())
                    .setPsNum(getPsNum())
                    .setInferenceScripts(getInferenceScripts())
                    .setInferenceMapFunc(getInferenceMapFunc())
                    .setInferenceHyperParams(getInferenceHyperParams())
                    .setInferenceEnvPath(getInferenceEnvPath())
                    .setInferenceSelectedCols(getInferenceSelectedCols())
                    .setInferenceOutputCols(getInferenceOutputCols())
                    .setInferenceOutputTypes(getInferenceOutputTypes());
            return model;
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    @Override
    public Params getParams() {
        return params;
    }
}
```



### 2. Issues & scenarios

List：

\[Issue-1][Flink-AI-Extended] **Would it support multiple trains or inferences in the same job?**

\[Issue-2][Flink-AI-Extended] **Would it support BatchTableEnvironment for batch process application?**

\[Issue-3][Flink-AI-Extended] **Would it support Flink-1.9 cause Flink-1.8 doesn't have table-ml-api?**

\[Issue-4][Flink-AI-Extended] **What is the difference between train and inference?**

\[Issue-5][Flink-AI-Extended] **What's the strategy when batch size does not evenly divide the input dataset size?**

\[Issue-6]][Flink-AI-Extended]\[**Bug**] **The streaming inference result needs to wait until the next query to write to sink.**

\[Issue-7][Flink-AI-Extended]\[**Bug**] **Exception when there is only InputTfExampleConfig but no OutputTfExampleConfig.**

\[Issue-8][Flink ML Pipeline] **How to restart a estimator from a pre-trained model?**

\[Issue-9][Flink ML Pipeline] **How to use pipeline for stream machine learning?**



Details：

#### \[Issue-1][Flink-AI-Extended] Would it support multiple trains or inferences in the same job?

目前在同一个Flink job中只能进行一次TFUtils.train()或inference()调用，若调用多次会出现"am table"重复注册、"am server"被关闭等错误。

```java
PipelineStage s = stages.get(i);
Transformer t;
boolean needFit = isStageNeedFit(s);
if (needFit) {
		t = ((Estimator) s).fit(tEnv, input);
} else {
		// stage is Transformer, guaranteed in appendStage() method
		t = (Transformer) s;
}
transformStages.add(t);
input = t.transform(tEnv, input); // get input for next stage
```

这样的使用场景是存在的，比如说在Flink-1.9的[Pipeline](https://github.com/apache/flink/blob/release-1.9/flink-ml-parent/flink-ml-api/src/main/java/org/apache/flink/ml/api/core/Pipeline.java)中（如上），我将Flink-AI-Extended封装成Estimator和Model来引入到Pipeline框架里，当调用fit(TableEnvironment, Table)时，estimator会先用上一轮输入的table训练得到一个model，然后再用该model来transfrom输入的table传给下一轮stage。这里就需要在同一个job中先调用TFUtils.train()然后再调用inference。

因此，我想问一下之后是否能支持多次train/inference调用？现在版本无法支持的原因主要在哪些地方？如果我想添加这一功能的话，应该从哪里下手好？

**Respone from c4mmmm**：需要大改



#### \[Issue-2][Flink-AI-Extended] Would it support BatchTableEnvironment for batch process application?

```java
TFUtils.train(streamEnv, null, config);
```

现在TFUtils的api中均需传入一个StreamExecutionEnvironment（如上），但如果table是注册在BatchTableEnvironment的话就无法获得所需的StreamExecutionEnvironment。

基于Batch的机器学习算法是比较常见的，反而完全基于Stream的场景不多。大部分算法的开发者都会有一个相对固定的数据集，并在该数据集上训练和评估。虽然可以通过一些方法把Dataset转成Stream，但这样也会强行要求整个ExecutionEnvironment切换到Stream上，对于用户来说不是太友好。而且，Table是无关batch/stream的，在TFUtils的api中强行绑定一个StreamExecutionEnvironment也有点不太合理。

因此，我想问一下能否也在api层面上支持BatchTableEnvironment？或者设计成与Environment无关，只需Table即可？不知道这样在实现上能否行得通。



#### \[Issue-3][Flink-AI-Extended] Would it support Flink-1.9 cause Flink-1.8 doesn't have table-ml-api?

现在的Flink-AI-Extended是基于Flink-1.8.0的，社区在6月份发布了1.8.1，在8月份发布了1.9并提出了新的基于Table的ML api。

该项目之后是否也会兼容1.8.1和1.9？特别是在1.9上不少api（比如Table、StreamExecutionEnvironment）发生了改变。



#### \[Issue-4][Flink-AI-Extended] What is the difference between train and inference?

see issue [@link](https://github.com/alibaba/flink-ai-extended/issues/13)

在TFUtils的api区分了train和inference，但两个接口在使用、参数、返回值上都基本一样，甚至在使用中随意替换也能正常运行，因此想问一下区分这两个api的原因是什么？

```java
public static <IN, OUT> DataStream<OUT> train(StreamExecutionEnvironment streamEnv, DataStream<IN> input,
			TFConfig tfConfig, TypeInformation<OUT> outTI) throws IOException;

public static <IN, OUT> DataStream<OUT> inference(StreamExecutionEnvironment streamEnv, DataStream<IN> input,
			TFConfig tfConfig, TypeInformation<OUT> outTI) throws IOException;
```

在我的想法里，Flink-AI-Extended只是扮演一个环境协调、任务调度的角色，而实际的运行模式应该下放到python管理并通过超参来配置。我认为可以用invoke(args…)替换掉现在的train/inference，这样在api层面上也更加的简洁，不知道在实现上有没有不合适的地方？



#### \[Issue-5][Flink-AI-Extended] What's the strategy when batch size does not evenly divide the input dataset size?

```python
dataset = context.flink_stream_dataset()
dataset = dataset.batch(3)
iterator = dataset.make_one_shot_iterator()
next_batch = iterator.get_next()
```

在python中获取从flink table输入的数据时，可以指定每次batch的大小（比如3）。但在批处理环境下，如果table是有界的，比如1000行，这时batch_size就不能整除dataset_size，导致最后一个batch一直在等待直到超时，程序也不能正常退出。在流处理环境下则一般不存在这个问题，因为把batch_size设成1也是合理的。

因此是否能添加一些策略来保证batch能正常获取，比如说：1.抛弃尾端无法达到一个batch大小的数据；2.用默认值补全成一个batch；3.设置超时机制，当超时后采用策略1或2，超时前继续等待。



#### \[Issue-6]][Flink-AI-Extended]\[Bug] The streaming inference result needs to wait until the next query to write to sink.

see issue [@link](https://github.com/alibaba/flink-ai-extended/issues/11)

流处理环境下，从source读取的数据经过Flink-AI-Extended处理后，并没有立即写到sink上，而是要等到source的下一条数据到达后才写入。

In the stream processing environment, after reading data from the source and processing it through Flink-AI-Extended, the result is **not immediately written to the sink**, but is not written **until the next data** of the source arrives.

I built a simple demo for stream processing. Source injects a message every 5 seconds, a total of 25. The python part is immediately written back to the sink after reading.
When I inject a message into the source, the log shows that the python process has received it and executed "context.output_writer_op" in python, but the sink did not receive any messages. When I continue to inject a message into the source, the last result is written to the sink.

The following is the log:

```
...
[Source][2019-07-31 11:45:56.76]produce data-10
[Sink][2019-07-31 11:45:56.76]finish data-9

[Source][2019-07-31 11:46:01.765]produce data-11
[Sink][2019-07-31 11:46:01.765]finish data-10
...
```

But I want to write back to sink immediately after executing "output_writer_op":

```
...
[Source][2019-07-31 11:45:56.76]produce data-10
[Sink][2019-07-31 11:45:56.76]finish data-10

[Source][2019-07-31 11:46:01.765]produce data-11
[Sink][2019-07-31 11:46:01.765]finish data-11
...
```

For the time being, it is not clear why it is the cause of this situation.

The following is my demo code:

```java
package org.apache.flink.table.ml.lib.tensorflow;

import com.alibaba.flink.ml.operator.util.DataTypes;
import com.alibaba.flink.ml.tensorflow.client.TFConfig;
import com.alibaba.flink.ml.tensorflow.client.TFUtils;
import com.alibaba.flink.ml.tensorflow.coding.ExampleCoding;
import com.alibaba.flink.ml.tensorflow.coding.ExampleCodingConfig;
import com.alibaba.flink.ml.tensorflow.util.TFConstants;
import com.alibaba.flink.ml.util.MLConstants;
import org.apache.curator.test.TestingServer;
import org.apache.flink.api.common.restartstrategy.RestartStrategies;
import org.apache.flink.api.common.state.ListState;
import org.apache.flink.api.common.state.ListStateDescriptor;
import org.apache.flink.api.common.typeinfo.BasicTypeInfo;
import org.apache.flink.api.common.typeinfo.TypeInformation;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.api.java.typeutils.RowTypeInfo;
import org.apache.flink.runtime.state.FunctionInitializationContext;
import org.apache.flink.runtime.state.FunctionSnapshotContext;
import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.source.SourceFunction;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.TableSchema;
import org.apache.flink.table.api.java.StreamTableEnvironment;
import org.apache.flink.table.ml.lib.tensorflow.util.Utils;
import org.apache.flink.types.Row;
import org.junit.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.sql.Timestamp;
import java.util.HashMap;
import java.util.Map;

public class SourceSinkTest {
    private static final String ZookeeperConn = "127.0.0.1:2181";
    private static final String[] Scripts = {"/Users/bodeng/TextSummarization-On-Flink/src/main/python/pointer-generator/test.py"};
    private static final int WorkerNum = 1;
    private static final int PsNum = 0;

    @Test
    public void testSourceSink() throws Exception {
        TestingServer server = new TestingServer(2181, true);
        StreamExecutionEnvironment streamEnv = StreamExecutionEnvironment.createLocalEnvironment(1);
        streamEnv.setRestartStrategy(RestartStrategies.noRestart());

        DataStream<Row> sourceStream = streamEnv.addSource(
                new DummyTimedSource(20, 5), new RowTypeInfo(Types.STRING)).setParallelism(1);
        StreamTableEnvironment tableEnv = StreamTableEnvironment.create(streamEnv);
        Table input = tableEnv.fromDataStream(sourceStream, "input");
        TFConfig config = createTFConfig("test_source_sink");

        TableSchema outputSchema = new TableSchema(new String[]{"output"}, new TypeInformation[]{BasicTypeInfo.STRING_TYPE_INFO});

        // configure encode coding
        String strInput = ExampleCodingConfig.createExampleConfigStr(
                new String[]{"input"}, new DataTypes[]{DataTypes.STRING},
                ExampleCodingConfig.ObjectType.ROW, Row.class);
        config.getProperties().put(TFConstants.INPUT_TF_EXAMPLE_CONFIG, strInput);
        config.getProperties().put(MLConstants.ENCODING_CLASS,
                ExampleCoding.class.getCanonicalName());

        // configure decode coding
        String strOutput = ExampleCodingConfig.createExampleConfigStr(
                new String[]{"output"}, new DataTypes[]{DataTypes.STRING},
                ExampleCodingConfig.ObjectType.ROW, Row.class);
        config.getProperties().put(TFConstants.OUTPUT_TF_EXAMPLE_CONFIG, strOutput);
        config.getProperties().put(MLConstants.DECODING_CLASS,
                ExampleCoding.class.getCanonicalName());
      
        Table output = TFUtils.inference(streamEnv, tableEnv, input, config, outputSchema);
        tableEnv.toAppendStream(output, Row.class)
                .map(r -> "[Sink][" + new Timestamp(System.currentTimeMillis()) + "]finish " + r.getField(0) + "\n")
                .print().setParallelism(1);

        streamEnv.execute();
        server.stop();
    }

    private TFConfig createTFConfig(String mapFunc) {
        Map<String, String> prop = new HashMap<>();
        prop.put(MLConstants.CONFIG_STORAGE_TYPE, MLConstants.STORAGE_ZOOKEEPER);
        prop.put(MLConstants.CONFIG_ZOOKEEPER_CONNECT_STR, ZookeeperConn);
        return new TFConfig(WorkerNum, PsNum, prop, Scripts, mapFunc, null);
    }

    private static class DummyTimedSource implements SourceFunction<Row>, CheckpointedFunction {
        public static final Logger LOG = LoggerFactory.getLogger(DummyTimedSource.class);
        private long count = 0L;
        private long MAX_COUNT;
        private long INTERVAL;
	    private volatile boolean isRunning = true;

        private transient ListState<Long> checkpointedCount;

        public DummyTimedSource(long maxCount, long interval) {
            this.MAX_COUNT = maxCount;
            this.INTERVAL = interval;
        }

        @Override
        public void run(SourceContext<Row> ctx) throws Exception {
            while (isRunning && count < MAX_COUNT) {
                // this synchronized block ensures that state checkpointing,
                // internal state updates and emission of elements are an atomic operation
                synchronized (ctx.getCheckpointLock()) {
                    Row row = new Row(1);
                    row.setField(0, String.format("data-%d", count));
                    System.out.println("[Source][" + new Timestamp(System.currentTimeMillis()) + "]produce " + row.getField(0));
                    ctx.collect(row);
                    count++;
                    Thread.sleep(INTERVAL * 1000);
                }
            }
        }

        @Override
        public void cancel() {
            isRunning = false;
        }

        @Override
        public void snapshotState(FunctionSnapshotContext context) throws Exception {
            this.checkpointedCount.clear();
            this.checkpointedCount.add(count);
        }

        @Override
        public void initializeState(FunctionInitializationContext context) throws Exception {
            this.checkpointedCount = context
                    .getOperatorStateStore()
                    .getListState(new ListStateDescriptor<>("count", Long.class));

            if (context.isRestored()) {
                for (Long count : this.checkpointedCount.get()) {
                    this.count = count;
                }
            }
        }
    }
}
```

and python code:

```python
import sys
import datetime

import tensorflow as tf
from flink_ml_tensorflow.tensorflow_context import TFContext


class FlinkReader(object):
    def __init__(self, context, batch_size=1, features={'input': tf.FixedLenFeature([], tf.string)}):
        self._context = context
        self._batch_size = batch_size
        self._features = features
        self._build_graph()

    def _decode(self, features):
        return features['input']

    def _build_graph(self):
        dataset = self._context.flink_stream_dataset()
        dataset = dataset.map(lambda record: tf.parse_single_example(record, features=self._features))
        dataset = dataset.map(self._decode)
        dataset = dataset.batch(self._batch_size)
        iterator = dataset.make_one_shot_iterator()
        self._next_batch = iterator.get_next()

    def next_batch(self, sess):
        try:
            batch = sess.run(self._next_batch)
            return batch
        except tf.errors.OutOfRangeError:
            return None


class FlinkWriter(object):
    def __init__(self, context):
        self._context = context
        self._build_graph()

    def _build_graph(self):
        self._write_feed = tf.placeholder(dtype=tf.string)
        self.write_op, self._close_op = self._context.output_writer_op([self._write_feed])

    def _example(self, results):
        example = tf.train.Example(features=tf.train.Features(
            feature={
                'output': tf.train.Feature(bytes_list=tf.train.BytesList(value=[results[0]])),
            }
        ))
        return example

    def write_result(self, sess, results):
        sess.run(self.write_op, feed_dict={self._write_feed: self._example(results).SerializeToString()})

    def close(self, sess):
        sess.run(self._close_op)



def test_source_sink(context):
    tf_context = TFContext(context)
    if 'ps' == tf_context.get_role_name():
        from time import sleep
        while True:
            sleep(1)
    else:
        index = tf_context.get_index()
        job_name = tf_context.get_role_name()
        cluster_json = tf_context.get_tf_cluster()
        cluster = tf.train.ClusterSpec(cluster=cluster_json)

        server = tf.train.Server(cluster, job_name=job_name, task_index=index)
        sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,
                                     device_filters=["/job:ps", "/job:worker/task:%d" % index])
        with tf.device(tf.train.replica_device_setter(worker_device='/job:worker/task:' + str(index), cluster=cluster)):
            reader = FlinkReader(tf_context)
            writer = FlinkWriter(tf_context)

            with tf.train.ChiefSessionCreator(master=server.target, config=sess_config).create_session() as sess:
                while True:
                    batch = reader.next_batch(sess)
                    if batch is None:
                        break
                    # tf.logging.info("[TF][%s]process %s" % (str(datetime.datetime.now()), str(batch)))

                    writer.write_result(sess, batch)
                writer.close(sess)
                sys.stdout.flush()
```



#### \[Issue-7][Flink-AI-Extended]\[Bug] Exception when there is only InputTfExampleConfig but no OutputTfExampleConfig.

see issue [@link](https://github.com/alibaba/flink-ai-extended/issues/10)

如下，这是正常配置ExampleCoding（即配置Flink与Python进行数据传输的格式），但如果只配置encode部分而不配置decode部分会无法执行，抛出异常。反之亦然。

```java
// configure encode example coding
String strInput = ExampleCodingConfig.createExampleConfigStr(encodeNames, encodeTypes, 
                                                             entryType, entryClass);
config.getProperties().put(TFConstants.INPUT_TF_EXAMPLE_CONFIG, strInput);
config.getProperties().put(MLConstants.ENCODING_CLASS,
                           ExampleCoding.class.getCanonicalName());

// configure decode example coding
String strOutput = ExampleCodingConfig.createExampleConfigStr(decodeNames, decodeTypes, 
                                                              entryType, entryClass);
config.getProperties().put(TFConstants.OUTPUT_TF_EXAMPLE_CONFIG, strOutput);
config.getProperties().put(MLConstants.DECODING_CLASS, 
                           ExampleCoding.class.getCanonicalName());
```

这样的使用场景是比较常见的，比如在训练过程中，用户只需要往TF传输数据，而无需其返回table，因此只会有flink-to-tf的encode阶段，而没有tf-to-flink的decode阶段。对用户来说，只设置encode相关的配置也是符合一般习惯的。

因此，我希望能只配置encode部分而不配置decode部分，相反的情况也同样如此。

经查阅，主要原因在与CodingFactory.java中的ReflectUtil.createInstance(className, classes, objects)方法。该方法会根据ENCODING_CLASS去创建一个ExampleCoding实例，而根据ExampleCoding.java的定义，在构造函数中会同时配置inputConfig和outputConfig（即便没有），这就会导致NullPointerException。

以下是异常信息：

```java
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.alibaba.flink.ml.util.ReflectUtil.createInstance(ReflectUtil.java:36)
	at com.alibaba.flink.ml.coding.CodingFactory.getEncoding(CodingFactory.java:49)
	at com.alibaba.flink.ml.data.DataExchange.<init>(DataExchange.java:58)
	at com.alibaba.flink.ml.operator.ops.MLMapFunction.open(MLMapFunction.java:80)
	at com.alibaba.flink.ml.operator.ops.MLFlatMapOp.open(MLFlatMapOp.java:51)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:36)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:102)
	at org.apache.flink.streaming.api.operators.StreamFlatMap.open(StreamFlatMap.java:43)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.openAllOperators(StreamTask.java:424)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:290)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:711)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at com.alibaba.flink.ml.tensorflow.coding.ExampleCodingConfig.fromJsonObject(ExampleCodingConfig.java:100)
	at com.alibaba.flink.ml.tensorflow.coding.ExampleCoding.<init>(ExampleCoding.java:57)
	... 16 more
```

以下是详细测试用例，Java代码：

```java
private static final String ZookeeperConn = "127.0.0.1:2181";
private static final String[] Scripts = {"test.py"};
private static final int WorkerNum = 1;
private static final int PsNum = 0;

@Test
public void testExampleCodingWithoutDecode() throws Exception {
		TestingServer server = new TestingServer(2181, true);
		StreamExecutionEnvironment streamEnv = 
      	StreamExecutionEnvironment.createLocalEnvironment(1);
		streamEnv.setRestartStrategy(RestartStrategies.noRestart());
		StreamTableEnvironment tableEnv = StreamTableEnvironment.create(streamEnv);
  
		Table input = tableEnv
				.fromDataStream(streamEnv.fromCollection(createDummyData()), "input");
		TableSchema inputSchema = 
				new TableSchema(new String[]{"input"}, 
                    		new TypeInformation[]{BasicTypeInfo.STRING_TYPE_INFO});
		TableSchema outputSchema = null;
  
  	TFConfig config = createTFConfig("test_example_coding_without_decode");
  	// configure encode coding
  	String strInput = ExampleCodingConfig.createExampleConfigStr(
      	new String[]{"input"}, new DataTypes[]{DataTypes.STRING}, 
      	ExampleCodingConfig.ObjectType.ROW, Row.class);
  	config.getProperties().put(TFConstants.INPUT_TF_EXAMPLE_CONFIG, strInput);
		config.getProperties().put(MLConstants.ENCODING_CLASS, 
                               ExampleCoding.class.getCanonicalName());
  
  	// run in python
		Table output = TFUtils.inference(streamEnv, tableEnv, input, config, outputSchema);

		streamEnv.execute();
		server.stop();
}

private List<Row> createDummyData() {
		List<Row> rows = new ArrayList<>();
		for (int i = 0; i < 10; i++) {
				Row row = new Row(1);
				row.setField(0, String.format("data-%d", i));
        rows.add(row);
		}
		return rows;
}

private TFConfig createTFConfig(String mapFunc) {
		Map<String, String> prop = new HashMap<>();
		prop.put(MLConstants.CONFIG_STORAGE_TYPE, MLConstants.STORAGE_ZOOKEEPER);
		prop.put(MLConstants.CONFIG_ZOOKEEPER_CONNECT_STR, ZookeeperConn);
		return new TFConfig(WorkerNum, PsNum, prop, Scripts, mapFunc, null);
}
```

Python代码：

```python
import tensorflow as tf
from flink_ml_tensorflow.tensorflow_context import TFContext


class FlinkReader(object):
    def __init__(self, context, batch_size=1, features={'input': tf.FixedLenFeature([], tf.string)}):
        self._context = context
        self._batch_size = batch_size
        self._features = features
        self._build_graph()

    def _decode(self, features):
        return features['input']

    def _build_graph(self):
        dataset = self._context.flink_stream_dataset()
        dataset = dataset.map(lambda record: tf.parse_single_example(record, features=self._features))
        dataset = dataset.map(self._decode)
        dataset = dataset.batch(self._batch_size)
        iterator = dataset.make_one_shot_iterator()
        self._next_batch = iterator.get_next()

    def next_batch(self, sess):
        try:
            batch = sess.run(self._next_batch)
            return batch
        except tf.errors.OutOfRangeError:
            return None


def test_example_coding_without_decode(context):
    tf_context = TFContext(context)
    if 'ps' == tf_context.get_role_name():
        from time import sleep
        while True:
            sleep(1)
    else:
        index = tf_context.get_index()
        job_name = tf_context.get_role_name()
        cluster_json = tf_context.get_tf_cluster()
        cluster = tf.train.ClusterSpec(cluster=cluster_json)

        server = tf.train.Server(cluster, job_name=job_name, task_index=index)
        sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,
                                     device_filters=["/job:ps", "/job:worker/task:%d" % index])
        with tf.device(tf.train.replica_device_setter(worker_device='/job:worker/task:' + str(index), cluster=cluster)):
            reader = FlinkReader(tf_context)

            with tf.train.ChiefSessionCreator(master=server.target, config=sess_config).create_session() as sess:
                while True:
                    batch = reader.next_batch(sess)
                    tf.logging.info(str(batch))
                    if batch is None:
                        break
                sys.stdout.flush()
```



#### \[Issue-8][Flink ML Pipeline] How to restart an estimator from a pre-trained model?

estimator是否可以从已经训练好的model或者另一个训练到一定程度estimator重新加载，然后接着继续训练？

我的想法中，这样的功能主要有两个应用场景：

1. 用户可能会使用别人预训练好的model对自己的数据集做fine-tuning，即estimator的参数并非自己随机初始化，而是从一个已有的model中加载。
2. 用户在训练一个复杂的模型时，训练的时间成本比较高，为了稳健起见可能希望在训练过程中有checkpoint机制，比如每10轮迭代保存一次，万一训练过程中意外中断了也可以从checkpoint中恢复并继续训练。

这些feature在大部分的深度学习框架中都是有的，传统机器学习算法一般没这样做，不知道这样的功能是否有必要呢？

**Respone from c4mmmm**：需要estimator自己实现



#### \[Issue-9][Flink ML Pipeline] How to use pipeline for stream machine learning?

现在的Pipeline设计以及大部分算法都是基于批处理的，虽然API层面没有强行要求，但一旦尝试使用StreamExecutionEnvionment就会无法使用。

这样的场景有很多，比如说：

1. 一个注册在StreamTableEnvironment的table，要传入estimator进行fit时，假如数据是无界的，fit过程应该什么时候停止？怎么停止？
2. 在StreamExecutionEnvionment下，有一个包含estimator的pipeline需要训练，estimator需要根据数据训练得到一个实例化的model，然后使用该model来transform数据。但是，在streamEnv.execute()之前，所以的操作只是构建执行的DAG，没有真正的数据流动，那么这个需要数据训练出来的model该如何实例化呢？
3. 如果我们使用窗口（window）的方法把流处理环境转换成批处理，那么应该在哪里window？怎么window合适？

这些问题不好解决，主要很少有一个明确的基于流的机器学习标准或算法以供参考指引。

而且有个比较要命的是，Pipeline现在只能用于批处理场景，而Flink-AI-Extended的API必须传入一个StreamExecutionEnvionment。

**Respone from c4mmmm**：

1 不停止，就是无限的训练,未来会有流上的迭代框架,会有相应的收敛时停止任务的机制。
2 实例化的model只包含meta，数据在estimator中写入外部存储，model里读取外部存储,和tf那个模式一样。
3 这个是纯粹的实现问题，这个不是流处理变成批处理，本质还是流处理。这个是处理逻辑了，即使开了window，流还是无限的，所以并不会改变estimator干的事情，对框架没有影响。

结合1，后面pipeline可能要改,不过得等迭代框架出来再看怎么改。

https://docs.google.com/document/d/1MIWrUW_X7Ag9IHDudFBsZkVUVNpV1kZI7LsFLSqibiI/edit
https://docs.google.com/document/d/1uy5YSHkegGCMuWr7_ggwLMUf0Em4DueGo59NNZRLdC0/edit#



### 3. Possible improvement

#### \[Bug fixed][PR to issue-6] -> [pull request](https://github.com/alibaba/flink-ai-extended/pull/15)

**Issue (see [@link](https://github.com/alibaba/flink-ai-extended/issues/11)):**

The streaming inference result needs to wait until the next query to write to sink.

**Reason:**

The main reason is that **MLMapFunction** and **Python** processing are in two **different threads**.

As shown in the figure, when **data1** is injected into the input stream, MLMapFunction.flatMap() is triggered, and the data is written to the input queue and then read from the output queue in the same thread. The **problem** is that another thread where python is located has **not finished** reading **data1** and writed it to the output queue. At this time, the MLMapFunction has finished reading and gets a null value. The result of **data1** can only be extracted when the next **data2** is injected.

Implementation details of  **flatMap()** can be found in com.alibaba.flink.ml.operator.ops.**MLMapFunction.java**

![design](https://raw.githubusercontent.com/LittleBBBo/TextSummarization-On-Flink/master/doc/github/Issue6/Issue6.bmp)

**Proposal (see [@link](https://github.com/alibaba/flink-ai-extended/issues/11#issuecomment-517212803)):**

As shown in the figure, try to read data continuously from the queue by adding a separate thread called **ConsumerThread**, and append to OUT immediately if there is data.

![design](https://raw.githubusercontent.com/LittleBBBo/TextSummarization-On-Flink/master/doc/github/Issue6/Issue6_PR.bmp)

Here is the original implementation details of **flatMap()** in **MLMapFunction.java**:

```java
/**
 * process input data and collect results.
 * @param value input object.
 * @param out output result.
 * @throws Exception
 */
void flatMap(IN value, Collector<OUT> out) throws Exception {
	collector = out;

	//put the read & write in a loop to avoid dead lock between write queue and read queue.
	boolean writeSuccess = false;
	do {
		drainRead(out, false);

		writeSuccess = dataExchange.write(value);
		if (!writeSuccess) {
			Thread.yield();
		}
	} while (!writeSuccess);
}

private void drainRead(Collector<OUT> out, boolean readUntilEOF) {
	while (true) {
		try {
			Object r = dataExchange.read(readUntilEOF);
			if (r != null) {
				out.collect((OUT) r);
			} else {
				break;
			}
		} catch (InterruptedIOException iioe) {
			LOG.info("{} Reading from is interrupted, canceling the server", mlContext.getIdentity());
			serverFuture.cancel(true);
		} catch (IOException e) {
			LOG.error("Fail to read data from python.", e);
		}
	}
}
```



#### \[Bug fixed][PR to issue-7] -> [pull request](https://github.com/alibaba/flink-ai-extended/pull/12)

**Issue (see [@link](https://github.com/alibaba/flink-ai-extended/issues/10)):**

Exception when there is only InputTfExampleConfig but no OutputTfExampleConfig 

**Reason:**

After review, the main reason is the method of **ReflectUtil.createInstance(className, classes, objects)** in **CodingFactory.java**. This method will create an **ExampleCoding** instance according to **ENCODING_CLASS**. According to the definition of **ExampleCoding.java**, both inputConfig and outputConfig(even if not) will be configured in the constructor, which will result in a NullPointerException.

**Proposal (see [@link](https://github.com/alibaba/flink-ai-extended/issues/10#issuecomment-516778157)):**

This issue can be fix by **adding null pointer checks** in the constructor of com.alibaba.flink.ml.tensorflow.coding.**ExampleCoding.java**

From:

```java
public ExampleCoding(MLContext mlContext) throws CodingException {
	this.mlContext = mlContext;
	this.inputConfig = new ExampleCodingConfig();
	JSONObject jsonObject = JSONObject.parseObject(mlContext.getProperties().get(INPUT_TF_EXAMPLE_CONFIG));
	this.inputConfig.fromJsonObject(jsonObject);
	this.outputConfig = new ExampleCodingConfig();
	JSONObject jsonObjectOutput = JSONObject.parseObject(mlContext.getProperties().get(OUTPUT_TF_EXAMPLE_CONFIG));
	this.outputConfig.fromJsonObject(jsonObjectOutput);
}
```

Modify to:

```java
public ExampleCoding(MLContext mlContext) throws CodingException {
	this.mlContext = mlContext;
	this.inputConfig = new ExampleCodingConfig();
	JSONObject jsonObject = JSONObject.parseObject(mlContext.getProperties().get(INPUT_TF_EXAMPLE_CONFIG));
	if (jsonObject != null) {
		this.inputConfig.fromJsonObject(jsonObject);
	}
	this.outputConfig = new ExampleCodingConfig();
	JSONObject jsonObjectOutput = JSONObject.parseObject(mlContext.getProperties().get(OUTPUT_TF_EXAMPLE_CONFIG));
	if (jsonObjectOutput != null) {
		this.outputConfig.fromJsonObject(jsonObjectOutput);
	}
}
```